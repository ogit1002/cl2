#IR_1
import warnings
warnings.filterwarnings('ignore')

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# Download necessary NLTK data
nltk.download('stopwords')
nltk.download('punkt')

# Sample text document
text_document = "Text preprocessing is an essential step in natural language processing"

# Tokenize the document into words and convert to lowercase
words = word_tokenize(text_document.lower())
print(words)

# Remove stop words
stop_words = set(stopwords.words('english'))
filtered_words = [word for word in words if word.lower() not in stop_words]
print(filtered_words)

# Apply stemming using PorterStemmer
stemmer = PorterStemmer()
stemmed_words = [stemmer.stem(word) for word in filtered_words]
print(stemmed_words)

# Apply lemmatization using WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]
print(lemmatized_words)


#IR_2
import warnings
warnings.filterwarnings('ignore')

import re
from collections import defaultdict

# Sample documents
documents = {
    1: "This is the first document. It contains some text.",
    2: "The second document is longer. It also contains some text.",
    3: "This is the third document. It is different from the first two."
}

# Function to preprocess documents (convert to lowercase, remove stopwords, tokenize)
def preprocess_document(doc):
    tokens = re.findall(r'\w+', doc.lower())  # Tokenize and convert to lowercase
    stop_words = set(["is", "the", "it", "and", "some"])  # Define stopwords
    tokens = [token for token in tokens if token not in stop_words]  # Remove stopwords
    return tokens

# Building the inverted index
inverted_index = defaultdict(list)
for doc_id, doc_text in documents.items():
    tokens = preprocess_document(doc_text)
    for token in tokens:
        inverted_index[token].append(doc_id)

# Function to retrieve documents based on a query
def retrieve_documents(query):
    query_tokens = preprocess_document(query)
    result = set()
    for token in query_tokens:
        if token in inverted_index:
            result.update(inverted_index[token])
    return list(result)

# Example query
query = "contains"
matching_documents = retrieve_documents(query)

# Output the matching documents
if matching_documents:
    print(f"Matching documents for query '{query}':")
    for doc_id in matching_documents:
        print(f"Document {doc_id}: {documents[doc_id]}")
else:
    print("No matching documents found.")


#IR_3
import pandas as pd
from pgmpy.models import BayesianNetwork
from pgmpy.estimators import MaximumLikelihoodEstimator
from pgmpy.inference import VariableElimination

# Load the dataset
df = pd.read_csv("/content/drive/My Drive/7th sem/IR/heart_dataset.csv")

# Display basic information about the dataset
print(df.head())
print(df.tail())
print(df.info())
print(df.describe())

# Define the Bayesian Network structure
model = BayesianNetwork([
    ('age', 'condition'),
    ('sex', 'condition'),
    ('cp', 'condition'),
    ('trestbps', 'condition'),
    ('chol', 'condition')
])

# Fit the model using Maximum Likelihood Estimator
model.fit(df, estimator=MaximumLikelihoodEstimator)

# Perform inference using Variable Elimination
HeartDiseasetest_infer = VariableElimination(model)

# Query 1: Probability of heart disease given chest pain type (cp = 3)
q1 = HeartDiseasetest_infer.query(variables=['condition'], evidence={'cp': 3})
print(q1)

# Query 2: Probability of heart disease given resting blood pressure (trestbps = 140)
q2 = HeartDiseasetest_infer.query(variables=['condition'], evidence={'trestbps': 140})
print(q2)

# Query 3: Probability of heart disease given age (age = 50)
q3 = HeartDiseasetest_infer.query(variables=['condition'], evidence={'age': 50})
print(q3)

# Query 4: Probability of heart disease given cholesterol level (chol = 220)
q4 = HeartDiseasetest_infer.query(variables=['condition'], evidence={'chol': 220})
print(q4)

#IR_4
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from warnings import filterwarnings
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
df = pd.read_csv("spam_mail.csv")
df.head()
df.isna().sum()
df = df.dropna(subset=['Body'])
df.shape
df.dtypes
df.describe()
df1=df
df1.duplicated().sum()
df2 = df1.drop_duplicates()
df2.duplicated().sum()
df2["Body"] = df2["Body"].replace("\n","",regex=True)
df2.head()
x_train,x_test,y_train,y_test = train_test_split(df2['Body'],df2['Label'],test_size=0.1)
x_train
v = CountVectorizer()
x_train_count = v.fit_transform(x_train)
x_test_count = v.transform(x_test)
x_train_count.toarray()
model = LogisticRegression()
model.fit(x_train_count,y_train)
model.score(x_test_count,y_test)
email = ['50% discount on data science courses signup now']
new_email = v.transform(email)
ans = model.predict(new_email)
if ans[0] == 1:
print("Spam")
else :
print("Not Spam")

#IR_5
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.cluster import AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
from scipy.cluster.hierarchy import dendrogram,linkage
from sklearn.decomposition import PCA
iris = load_iris()
X = iris.data
y = iris.target
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
n_clusters = 3
agg_clustering = AgglomerativeClustering(n_clusters=n_clusters,linkage='ward')
cluster_labels = agg_clustering.fit_predict(X_pca)
linkage_matrix = linkage(X_pca, method='ward')
# linked = linkage(X_pca,'ward')
plt.figure(figsize=(12,6))
dendrogram(linkage_matrix)
plt.title("Agglomerative Hierarchical Clustering")
plt.show()
print("Cluster Labels:")
print(cluster_labels)
plt.scatter(X_pca[:,0] , X_pca[:,1], c = cluster_labels, cmap='rainbow')
plt.xlabel('Principal component 1')
plt.ylabel('Principal component 2')
plt.title('Agglomerative Hierarchical Clustering')
plt.show()


#BI_4
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.ensemble import RandomForestClassifier
import warnings
warnings.filterwarnings("ignore")
# Load the dataset
df = pd.read_csv("polymerase_cluster.csv")
# Extract feature columns (assuming they are named as '0', '1', ..., '29')
X = df[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',
        '11', '12', '13', '14', '15', '16', '17', '18', '19', '20',
        '21', '22', '23', '24', '25', '26', '27', '28', '29']]
# Assuming 'G' columns represent classes, combine them into a single target variable
# Convert multi-class one-hot encoding into a single categorical target
df['target'] = df[['G1', 'G2', 'G3', 'G4', 'G5', 'G6', 'G7', 'G8', 'G9', 'G10']].idxmax(axis=1)
y = df['target']
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Initialize and train the RandomForestClassifier
rfc = RandomForestClassifier(random_state=42)
rfc.fit(X_train, y_train)
# Make predictions on the test set
y_pred = rfc.predict(X_test)
# Calculate accuracy and print the classification report
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
cr = classification_report(y_test, y_pred)
print("Classification report:\n", cr)
# Define a new input array (should match the number of features, 30 in this case)
# Example: Custom input for a single prediction
custom_input = [[0.5, 1.2, -0.3, 2.1, 3.2, 0.1, -1.0, 0.4, 1.8, -0.5,
                 0.2, 2.3, -1.2, 1.1, 0.5, 1.7, 2.6, -0.7, 1.0, -0.3,
                 0.6, -1.4, 1.1, -0.8, 0.5, 1.9, 0.2, -1.6, 2.3, 0.9]]
# Predict with custom input
custom_pred = rfc.predict(custom_input)
print("Prediction for custom input:", custom_pred)

#BI_3
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
df_train = pd.read_csv('train.csv')
print(df_train.shape)
# Load the testing data
df_test = pd.read_csv('test.csv')
print(df_test.shape)
df_train.head()
df_test.head()
X_train = df_train.iloc[:, :-1]
y_train = df_train.iloc[:, -1]
X_test = df_test.iloc[:, :-1]
y_test = df_test.iloc[:, -1]
X_train.head()
y_train.head()
rf_model = RandomForestClassifier(random_state=8)
rf_model.fit(X_train, y_train.values)
rf_pred = rf_model.predict(X_test)
accuracy_score(y_test, rf_pred)
confusion_matrix(y_test,rf_pred)

#BI_2
pip install pandas numpy scipy statsmodels biopython

import numpy as np
import pandas as pd

import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

np.random.seed(42)

# Step 1: Simulate RNA-Seq Dataset
genes = [f"gene_{i}" for i in range(1, 181)]
conditions = ["Control", "Treatment"]
samples = [f"sample_{i}" for i in range(1, 11)]

data = np.random.poisson(lam=20, size=(100, 10))

# Simulate differential expression for some genes in Treatment condition
data[8:15, 5:10] += 15

# Create DataFrame
df = pd.DataFrame(data, index=genes, columns=samples)

# Step 2: Normalize the Data
df_norm = df.div(df.sum(axis=1), axis=0) * 1e6
df_log = np.log2(df_norm + 1)

def differential_expression(df, metadata):
    results = []

    for gene in df.index:
        y = df.loc[gene].values
        X = pd.get_dummies(metadata['condition'], drop_first=True)
        X = sm.add_constant(X.astype(float))
        model = sm.OLS(y, X).fit()
        p_value = model.pvalues[1]
        results.append({'gene': gene, 'p_value': p_value})

    results_df = pd.DataFrame(results)
    results_df['adjusted_p_value'] = sm.stats.multipletests(results_df['p_value'], method='fdr_bh')[1]

    return results_df

# Call the differential expression function to calculate results
results_df = differential_expression(df_log, metadata)

# Filter differentially expressed genes
deg = results_df[results_df['adjusted_p_value'] < 0.05]

# Step 4: Functional Annotation (Simulated Annotations)
annotations = {
    "gene_1": "Pathway A",
    "gene_2": "Pathway B",
    "gene_3": "Pathway C",
    "gene_4": "Pathway D",
    "gene_5": "Pathway E"
}
deg["annotation"] = deg['gene'].map(annotations).fillna("Unknown")
# Step 5: Biological Interpretation (Plotting)
plt.figure(figsize=(18, 6))

sns.scatterplot(x='gene', y='adjusted_p_value', hue='annotation', data=deg)
plt.xlabel('Genes')
plt.ylabel('Adjusted P-Value')
plt.title('Differentially Expressed Genes')
plt.xticks(rotation=90)

plt.legend(title='Annotations')
plt.tight_layout()
plt.show()

# Save results to a CSV file
deg.to_csv('differentially_expressed_genes.csv', index=False)

# Generate the Report

# RNA-Seq Data Analysis Report
report = """
Differentially Expressed Genes

{}

Functional Annotations

{}

Potential Biological Interpretations

The genes gene_1, gene_2, etc., are involved in pathways A, B, etc.
These pathways are important for understanding the effect of the treatment condition.

"""

report = report.format(deg[['gene', 'adjusted_p_value']], deg[['gene', 'annotation']])

# Save the report to a text file
with open('RNASeq_Analysis_Report.txt', 'w') as f:
    f.write(report)

print("Analysis complete. Results saved to 'differentially_expressed_genes.csv' and 'RNASeq_Analysis_Report.txt'.")

#BI_1
# Read the DNA sequence from a file
with open("dna_sequence.txt", "r") as file: 
    dna_sequence = file.read().strip()
# Calculate GC content
gc_content = (dna_sequence.count("G") + dna_sequence.count("C")) / 
len(dna_sequence) * 100
print("GC Content:", gc_content, "%")
# Find motifsmotif_to_find = "ATG"
motifs_found = [str(i) for i in range(len(dna_sequence)) if dna_sequence.startswith(motif_to_find, i)]
print("Motif '{}' found at positions: {}".format(motif_to_find, ",".join(motifs_found)) if motifs_found
else "Motif '{}' not found in the sequence.".format(motif_to_find))
# Identify coding regions
start_codon = "ATG"
stop_codons = ["TAA", "TAG", "TGA"]
coding_regions = []
for i, codon in enumerate(dna_sequence): 
    if dna_sequence[i:i+3] == start_codon: 
        for j in range(i+3, len(dna_sequence), 3): 
            codon = dna_sequence[j:j+3] 
            if codon in stop_codons: 
                coding_regions.append((i, j + 3)) 
                break 
            else: 
                continue
if coding_regions: 
    print("Coding regions:") 
    for start, end in coding_regions: 
        print("Start:", start, "End:", end)
else: 
    print("No coding regions found in the sequence.")
